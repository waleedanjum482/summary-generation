{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaCZZOir7YJj",
        "outputId": "5476c734-cfb5-42c0-adf3-b1e60d321171"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6jM6STI7er7",
        "outputId": "b379b895-1c97-42e1-9b90-45f7b7e27fea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text extraction and removing un-necessary patterns and removal of introduction etc. functions\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    pdf_file = open(pdf_file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    extracted_text = ''\n",
        "    for page in pdf_reader.pages:\n",
        "        extracted_text += page.extract_text()\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "\n",
        "def remove_pattern(text, pattern):\n",
        "    # Use regular expression to remove the specified pattern\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_text_before_pattern(text, pattern):\n",
        "    start_index = text.find(pattern)\n",
        "    if start_index != -1:\n",
        "        cleaned_text = text[start_index:]\n",
        "    else:\n",
        "        cleaned_text = text\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def count_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len(words)\n"
      ],
      "metadata": {
        "id": "D8ZfQjbR66T_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text = extract_text_from_pdf(\"/content/crime-and-punishment.pdf\")\n",
        "# print(extracted_text)\n",
        "\n",
        "pattern_to_remove = r'Free eBooks at Planet eBook\\.com'\n",
        "cleaned_text = remove_pattern(extracted_text, pattern_to_remove)\n",
        "# print(cleaned_text)\n",
        "\n",
        "pattern_to_find = \"\u0018Part I\u0018 Chapter I\"\n",
        "fcleaned_text = remove_text_before_pattern(cleaned_text, pattern_to_find)\n",
        "# print(fcleaned_text)\n",
        "\n",
        "total_words = count_words(fcleaned_text)"
      ],
      "metadata": {
        "id": "sHl03p7aBGMk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6bsz7CPKFtC",
        "outputId": "b7409667-27c3-461d-b293-7c73014f3f61"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dAH_XSANyB9g"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "e935VlsouRzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ozZyWC2xvWs"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"falcon-llm-7b\")\n",
        "# Specify the model name\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# As required pages are max 20(max 6000 words will come on 20 pages) so after\n",
        "# text extraction count total no of words and set the desired words to 5900 and after that set the model max_length\n",
        "# that is the output summary generation parameter to the value so that final summary is not exceeded to six pages.\n",
        "N0 = total_words\n",
        "N_desired = 5900\n",
        "# t is no of times i want to pass the summary throgh model.\n",
        "t = 3\n",
        "r = 1 - (N_desired / N0) ** (1/t)\n",
        "\n",
        "m_length = int(r*4000)\n",
        "\n",
        "def summary_generator(text_chunk):\n",
        "    text = \"\"\"{text_chunk}\"\"\"\n",
        "    # Generate the summary\n",
        "    summary = model.generate(\n",
        "        input_text=text,\n",
        "        max_length=m_length,\n",
        "        num_beams=5,\n",
        "        temperature=0.8,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return tokenizer.decode(summary[0], skip_special_tokens=True)\n",
        "    # print(tokenizer.decode(summary[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# funcition will split the text into chunks of 4000 so that model max token getting limit not exceeded.\n",
        "def split_text_into_chunks(text, chunk_size):\n",
        "    # Split the text into chunks of the specified size\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# chunk_size = 4000\n"
      ],
      "metadata": {
        "id": "r6U8TCkzkspp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first time generates summary of full text, then send this summary again to model to further summarize it. repeat the procedure three times on updated summaries so that meaningfull and good summary generates and words also not exceed to 6000."
      ],
      "metadata": {
        "id": "7HWIT7liNuso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks\n",
        "text_chunks = split_text_into_chunks(fcleaned_text, 4000)\n",
        "\n",
        "first_summary_text =\"\"\n",
        "for chunk in text_chunks:\n",
        "    summary = summary_generator(chunk)\n",
        "    first_summary_text+ = summary\n"
      ],
      "metadata": {
        "id": "upXZsO2hkuFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks\n",
        "text_chunks = split_text_into_chunks(first_summary_text, 4000)\n",
        "\n",
        "second_summary_text =\"\"\n",
        "for chunk in text_chunks:\n",
        "    summary = summary_generator(chunk)\n",
        "    second_summary_text+ = summary"
      ],
      "metadata": {
        "id": "iJ1xSMT7lG3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = split_text_into_chunks(second_summary_text, 4000)\n",
        "\n",
        "last_summary_text =\"\"\n",
        "for chunk in text_chunks:\n",
        "    summary = summary_generator(chunk)\n",
        "    last_summary_text+ = summary"
      ],
      "metadata": {
        "id": "L5TAhoADwMpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install reportlab"
      ],
      "metadata": {
        "id": "5bUBLwJkyolj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to docs with alignment and justification. then docx to the pdf.\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.enum.text import WD_ALIGN\n",
        "from docx2pdf import convert\n",
        "# Create a new DOCX document\n",
        "doc = Document()\n",
        "# Set the font style to Times New Roman and the font size to 20 points\n",
        "font = doc.styles['Normal'].font\n",
        "font.name = 'Times New Roman'\n",
        "font.size = Pt(12)\n",
        "paragraphs = last_summary_text.strip().split('\\n\\n')\n",
        "# Add each paragraph to the document with justified alignment\n",
        "for paragraph_text in paragraphs:\n",
        "    p = doc.add_paragraph(paragraph_text.strip(), style='Normal')\n",
        "    p.alignment = WD_ALIGN.JUSTIFY\n",
        "\n",
        "# Save the document to a DOCX file\n",
        "doc.save('final_summary.docx')\n",
        "convert(\"/content/final_summary.docx\")"
      ],
      "metadata": {
        "id": "GI4VJtjGypHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}